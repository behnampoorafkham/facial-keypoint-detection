{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from workspace_utils import active_session\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from models import Net\n",
    "\n",
    "from torch.utils.data import DataLoader , Dataset\n",
    "from torchvision import transforms , utils\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=18432, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (fc3): Linear(in_features=512, out_features=136, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Define the Net in models.py\n",
    "## Once you've define the network, you can instantiate it\n",
    "# one example conv layer has been provided for you\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Transform the dataset \n",
    "\n",
    "To prepare for training, we have created a transformed dataset of images and keypoints.\n",
    "\n",
    "### Define a data transform\n",
    "\n",
    "In PyTorch, a convolutional neural network expects a torch image of a consistent size as input. For efficient training, and so our model's loss does not blow up during training, it is also suggested that we normalize the input images and keypoints. The necessary transforms have been defined in `data_load.py` and we **do not** need to modify these.\n",
    "\n",
    "To define the data transform below, we have used a [composition](http://pytorch.org/tutorials/beginner/data_loading_tutorial.html#compose-transforms) of:\n",
    "1. Rescaling and/or cropping the data, such that we are left with a square image (the suggested size is 224x224px)\n",
    "2. Normalizing the images and keypoints; turning each RGB image into a grayscale image with a color range of [0, 1] and transforming the given keypoints into a range of [-1, 1]\n",
    "3. Turning these images and keypoints into Tensors\n",
    "\n",
    "**This transform will be applied to the training data and, later, the test data**. It will change how we go about displaying these images and keypoints, but these steps are essential for efficient training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Load import FacialKeypointsDataset\n",
    "from Load import Rescale , RandomCrop , Normalize , ToTensor\n",
    "data_transform = transforms.Compose([Rescale(250),\n",
    "                                    RandomCrop(224),\n",
    "                                    Normalize(),\n",
    "                                    ToTensor()])\n",
    "\n",
    "# testing that you've defined a transform\n",
    "assert(data_transform is not None), 'Define a data_transform'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images:  3462\n",
      "0 torch.Size([1, 24, 115]) torch.Size([68, 2])\n",
      "1 torch.Size([1, 77, 146]) torch.Size([68, 2])\n",
      "2 torch.Size([1, 106, 71]) torch.Size([68, 2])\n",
      "3 torch.Size([1, 149, 125]) torch.Size([68, 2])\n"
     ]
    }
   ],
   "source": [
    "# create the transformed dataset\n",
    "transformed_dataset = FacialKeypointsDataset(csv_file='data/training_frames_keypoints.csv',\n",
    "                                             root_dir='data/training/',\n",
    "                                             transform=data_transform)\n",
    "\n",
    "\n",
    "print('Number of images: ', len(transformed_dataset))\n",
    "\n",
    "# iterate through the transformed dataset and print some stats about the first few samples\n",
    "for i in range(4):\n",
    "    sample = transformed_dataset[i]\n",
    "    print(i, sample['image'].size(), sample['keypoints'].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching and loading data\n",
    "\n",
    "Next, having defined the transformed dataset, we can use PyTorch's DataLoader class to load the training data in batches of whatever size as well as to shuffle the data for training the model. You can read more about the parameters of the DataLoader in [this documentation](http://pytorch.org/docs/master/data.html).\n",
    "\n",
    "#### Batch size\n",
    "Decide on a good batch size for training your model. Try both small and large batch sizes and note how the loss decreases as the model trains. Too large a batch size may cause your model to crash and/or run out of memory while training.\n",
    "\n",
    "**Note for Windows users**: Please change the `num_workers` to 0 or you may face some issues with your DataLoader failing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': tensor([[[[1.6657e-04, 8.1730e-05, 5.1762e-05,  ..., 1.6602e-04,\n",
      "           1.7463e-04, 1.7472e-04],\n",
      "          [1.7204e-04, 9.9935e-05, 8.3434e-05,  ..., 1.5433e-04,\n",
      "           1.6679e-04, 1.6858e-04],\n",
      "          [1.7249e-04, 7.1911e-05, 4.7295e-05,  ..., 1.3649e-04,\n",
      "           1.3649e-04, 1.3649e-04],\n",
      "          ...,\n",
      "          [2.8859e-04, 2.8910e-04, 2.8909e-04,  ..., 1.7569e-03,\n",
      "           1.8124e-03, 1.8473e-03],\n",
      "          [2.7890e-04, 2.8394e-04, 2.8227e-04,  ..., 1.4578e-03,\n",
      "           1.6551e-03, 1.8323e-03],\n",
      "          [2.6475e-04, 2.6475e-04, 2.7636e-04,  ..., 1.2775e-03,\n",
      "           1.3026e-03, 1.4614e-03]]]]), 'keypoints': tensor([[[-3.4061, -2.6481],\n",
      "         [-3.3763, -2.5128],\n",
      "         [-3.3763, -2.3926],\n",
      "         [-3.3465, -2.2574],\n",
      "         [-3.2869, -2.1072],\n",
      "         [-3.1827, -1.9719],\n",
      "         [-3.0635, -1.8818],\n",
      "         [-2.9294, -1.7766],\n",
      "         [-2.8102, -1.7165],\n",
      "         [-2.6761, -1.7466],\n",
      "         [-2.6463, -1.7766],\n",
      "         [-2.6463, -1.8367],\n",
      "         [-2.6165, -1.9269],\n",
      "         [-2.5867, -2.0921],\n",
      "         [-2.5569, -2.2123],\n",
      "         [-2.5271, -2.3175],\n",
      "         [-2.5569, -2.4678],\n",
      "         [-3.0635, -2.7983],\n",
      "         [-2.9294, -2.8284],\n",
      "         [-2.8698, -2.8284],\n",
      "         [-2.8102, -2.8284],\n",
      "         [-2.7506, -2.7983],\n",
      "         [-2.5867, -2.7382],\n",
      "         [-2.5569, -2.7232],\n",
      "         [-2.5271, -2.7232],\n",
      "         [-2.5271, -2.6931],\n",
      "         [-2.5569, -2.6330],\n",
      "         [-2.6463, -2.6030],\n",
      "         [-2.6165, -2.5429],\n",
      "         [-2.5867, -2.4377],\n",
      "         [-2.5569, -2.3776],\n",
      "         [-2.7506, -2.2875],\n",
      "         [-2.6761, -2.2875],\n",
      "         [-2.6463, -2.2574],\n",
      "         [-2.6165, -2.2574],\n",
      "         [-2.6165, -2.2574],\n",
      "         [-2.9592, -2.6781],\n",
      "         [-2.8698, -2.6781],\n",
      "         [-2.8400, -2.6781],\n",
      "         [-2.8102, -2.6030],\n",
      "         [-2.8400, -2.6030],\n",
      "         [-2.8996, -2.6330],\n",
      "         [-2.6463, -2.5729],\n",
      "         [-2.6165, -2.5729],\n",
      "         [-2.5569, -2.5729],\n",
      "         [-2.5867, -2.5429],\n",
      "         [-2.5867, -2.5429],\n",
      "         [-2.6165, -2.5429],\n",
      "         [-2.8698, -2.0921],\n",
      "         [-2.7804, -2.1522],\n",
      "         [-2.6761, -2.1522],\n",
      "         [-2.6463, -2.1522],\n",
      "         [-2.6463, -2.1522],\n",
      "         [-2.6165, -2.0921],\n",
      "         [-2.6463, -2.0320],\n",
      "         [-2.6463, -2.0020],\n",
      "         [-2.6463, -1.9719],\n",
      "         [-2.7059, -1.9719],\n",
      "         [-2.7506, -2.0020],\n",
      "         [-2.8102, -2.0320],\n",
      "         [-2.8698, -2.0921],\n",
      "         [-2.7059, -2.1222],\n",
      "         [-2.6761, -2.0921],\n",
      "         [-2.6463, -2.0921],\n",
      "         [-2.6463, -2.0320],\n",
      "         [-2.6761, -2.0320],\n",
      "         [-2.7059, -2.0320],\n",
      "         [-2.7506, -2.0621]]], dtype=torch.float64)}\n",
      "{'image': tensor([[[[2.8227e-03, 3.2516e-03, 3.2615e-03,  ..., 3.4165e-03,\n",
      "           3.9113e-03, 3.9098e-03],\n",
      "          [2.8118e-03, 3.3769e-03, 3.7429e-03,  ..., 3.2216e-03,\n",
      "           3.9055e-03, 3.9156e-03],\n",
      "          [2.7602e-03, 3.4538e-03, 3.9033e-03,  ..., 2.8840e-03,\n",
      "           3.5107e-03, 3.6899e-03],\n",
      "          ...,\n",
      "          [3.9060e-03, 3.9016e-03, 3.5818e-03,  ..., 3.0171e-04,\n",
      "           4.0607e-06, 1.0032e-05],\n",
      "          [2.4813e-03, 2.2780e-03, 1.8592e-03,  ..., 3.5838e-04,\n",
      "           1.1656e-05, 2.1315e-06],\n",
      "          [2.0702e-04, 1.7890e-04, 4.5169e-04,  ..., 3.8217e-04,\n",
      "           2.5456e-05, 0.0000e+00]]]]), 'keypoints': tensor([[[-1.4849, -2.8764],\n",
      "         [-1.4571, -2.7996],\n",
      "         [-1.4154, -2.7227],\n",
      "         [-1.4015, -2.6459],\n",
      "         [-1.3598, -2.5947],\n",
      "         [-1.2902, -2.5306],\n",
      "         [-1.2485, -2.4922],\n",
      "         [-1.1651, -2.4538],\n",
      "         [-1.0539, -2.4282],\n",
      "         [-0.9427, -2.4666],\n",
      "         [-0.8871, -2.5306],\n",
      "         [-0.8454, -2.5819],\n",
      "         [-0.8037, -2.6587],\n",
      "         [-0.7759, -2.7355],\n",
      "         [-0.7759, -2.8124],\n",
      "         [-0.7620, -2.8892],\n",
      "         [-0.7620, -2.9660],\n",
      "         [-1.4154, -2.9532],\n",
      "         [-1.3737, -2.9660],\n",
      "         [-1.3180, -2.9660],\n",
      "         [-1.2763, -2.9660],\n",
      "         [-1.2346, -2.9532],\n",
      "         [-1.0539, -2.9660],\n",
      "         [-1.0122, -2.9788],\n",
      "         [-0.9705, -2.9788],\n",
      "         [-0.9010, -3.0045],\n",
      "         [-0.8593, -3.0045],\n",
      "         [-1.1234, -2.8892],\n",
      "         [-1.1095, -2.8380],\n",
      "         [-1.1095, -2.7740],\n",
      "         [-1.0956, -2.7227],\n",
      "         [-1.1512, -2.7227],\n",
      "         [-1.1234, -2.6971],\n",
      "         [-1.0956, -2.6971],\n",
      "         [-1.0678, -2.7227],\n",
      "         [-1.0261, -2.7355],\n",
      "         [-1.3320, -2.8892],\n",
      "         [-1.2902, -2.8892],\n",
      "         [-1.2485, -2.9148],\n",
      "         [-1.2068, -2.8892],\n",
      "         [-1.2485, -2.8764],\n",
      "         [-1.2902, -2.8764],\n",
      "         [-1.0261, -2.9020],\n",
      "         [-0.9844, -2.9276],\n",
      "         [-0.9427, -2.9276],\n",
      "         [-0.9010, -2.9276],\n",
      "         [-0.9427, -2.9020],\n",
      "         [-0.9844, -2.9020],\n",
      "         [-1.2346, -2.6459],\n",
      "         [-1.1929, -2.6459],\n",
      "         [-1.1095, -2.6587],\n",
      "         [-1.0956, -2.6587],\n",
      "         [-1.0539, -2.6843],\n",
      "         [-0.9844, -2.6843],\n",
      "         [-0.9427, -2.6843],\n",
      "         [-0.9844, -2.6203],\n",
      "         [-1.0261, -2.6075],\n",
      "         [-1.0678, -2.5819],\n",
      "         [-1.1234, -2.5819],\n",
      "         [-1.1651, -2.6075],\n",
      "         [-1.2346, -2.6459],\n",
      "         [-1.1234, -2.6459],\n",
      "         [-1.0956, -2.6459],\n",
      "         [-1.0539, -2.6587],\n",
      "         [-0.9427, -2.6843],\n",
      "         [-1.0539, -2.6459],\n",
      "         [-1.0956, -2.6203],\n",
      "         [-1.1234, -2.6203]]], dtype=torch.float64)}\n",
      "{'image': tensor([[[[4.8594e-04, 3.3103e-04, 1.7345e-04,  ..., 1.8055e-05,\n",
      "           2.1745e-05, 2.7082e-05],\n",
      "          [1.3216e-03, 9.0520e-04, 5.4275e-04,  ..., 1.8055e-05,\n",
      "           2.1745e-05, 2.7082e-05],\n",
      "          [2.0060e-03, 1.5322e-03, 8.7951e-04,  ..., 1.8055e-05,\n",
      "           2.1745e-05, 2.7082e-05],\n",
      "          ...,\n",
      "          [3.9255e-04, 3.5733e-04, 5.2503e-04,  ..., 5.1439e-04,\n",
      "           5.4694e-04, 5.6787e-04],\n",
      "          [4.4129e-04, 4.8108e-04, 7.1174e-04,  ..., 6.1569e-04,\n",
      "           6.2040e-04, 6.6146e-04],\n",
      "          [4.5783e-04, 4.5360e-04, 6.5979e-04,  ..., 6.0330e-04,\n",
      "           6.4846e-04, 6.4317e-04]]]]), 'keypoints': tensor([[[-1.5207, -1.5027],\n",
      "         [-1.5680, -1.3482],\n",
      "         [-1.5680, -1.1936],\n",
      "         [-1.5207, -1.0700],\n",
      "         [-1.5207, -0.9155],\n",
      "         [-1.4891, -0.7918],\n",
      "         [-1.4418, -0.6682],\n",
      "         [-1.3944, -0.5909],\n",
      "         [-1.2366, -0.5136],\n",
      "         [-1.0315, -0.5445],\n",
      "         [-0.8421, -0.5909],\n",
      "         [-0.6685, -0.6991],\n",
      "         [-0.5107, -0.8536],\n",
      "         [-0.3845, -1.0082],\n",
      "         [-0.3529, -1.1627],\n",
      "         [-0.3056, -1.3327],\n",
      "         [-0.3056, -1.4873],\n",
      "         [-1.5680, -1.6573],\n",
      "         [-1.5680, -1.7036],\n",
      "         [-1.5207, -1.7036],\n",
      "         [-1.4418, -1.6573],\n",
      "         [-1.3629, -1.6264],\n",
      "         [-1.0788, -1.5336],\n",
      "         [-0.9999, -1.5800],\n",
      "         [-0.9210, -1.5800],\n",
      "         [-0.7948, -1.5800],\n",
      "         [-0.6685, -1.5336],\n",
      "         [-1.2366, -1.3791],\n",
      "         [-1.3155, -1.2709],\n",
      "         [-1.3629, -1.1473],\n",
      "         [-1.3629, -1.0700],\n",
      "         [-1.3629, -1.0700],\n",
      "         [-1.3629, -1.0236],\n",
      "         [-1.2840, -0.9927],\n",
      "         [-1.2366, -1.0236],\n",
      "         [-1.1577, -1.0236],\n",
      "         [-1.4891, -1.5027],\n",
      "         [-1.4891, -1.5027],\n",
      "         [-1.3944, -1.5027],\n",
      "         [-1.3155, -1.4564],\n",
      "         [-1.3944, -1.4564],\n",
      "         [-1.4418, -1.4718],\n",
      "         [-1.0315, -1.4255],\n",
      "         [-0.9526, -1.4100],\n",
      "         [-0.9210, -1.4100],\n",
      "         [-0.7948, -1.3791],\n",
      "         [-0.8737, -1.3791],\n",
      "         [-0.9526, -1.3791],\n",
      "         [-1.4418, -0.9155],\n",
      "         [-1.3944, -0.9155],\n",
      "         [-1.3629, -0.9155],\n",
      "         [-1.3155, -0.9155],\n",
      "         [-1.2840, -0.9000],\n",
      "         [-1.1577, -0.8691],\n",
      "         [-1.0788, -0.8227],\n",
      "         [-1.1577, -0.7918],\n",
      "         [-1.2366, -0.7455],\n",
      "         [-1.3155, -0.7455],\n",
      "         [-1.3629, -0.7918],\n",
      "         [-1.3944, -0.8227],\n",
      "         [-1.4418, -0.9155],\n",
      "         [-1.3629, -0.8691],\n",
      "         [-1.3155, -0.8691],\n",
      "         [-1.2366, -0.8691],\n",
      "         [-1.0788, -0.8227],\n",
      "         [-1.2366, -0.8227],\n",
      "         [-1.3155, -0.8227],\n",
      "         [-1.3629, -0.8691]]], dtype=torch.float64)}\n",
      "{'image': tensor([[[[0.0023, 0.0024, 0.0025,  ..., 0.0023, 0.0023, 0.0023],\n",
      "          [0.0023, 0.0024, 0.0024,  ..., 0.0023, 0.0023, 0.0024],\n",
      "          [0.0023, 0.0024, 0.0024,  ..., 0.0026, 0.0027, 0.0028],\n",
      "          ...,\n",
      "          [0.0008, 0.0008, 0.0008,  ..., 0.0005, 0.0005, 0.0005],\n",
      "          [0.0008, 0.0008, 0.0008,  ..., 0.0005, 0.0005, 0.0005],\n",
      "          [0.0008, 0.0008, 0.0008,  ..., 0.0005, 0.0005, 0.0005]]]]), 'keypoints': tensor([[[-1.4951, -1.8461],\n",
      "         [-1.4814, -1.7678],\n",
      "         [-1.4540, -1.6894],\n",
      "         [-1.4403, -1.6372],\n",
      "         [-1.4128, -1.5458],\n",
      "         [-1.3717, -1.4675],\n",
      "         [-1.3580, -1.4283],\n",
      "         [-1.3169, -1.3892],\n",
      "         [-1.2209, -1.3631],\n",
      "         [-1.0975, -1.3761],\n",
      "         [-1.0152, -1.4153],\n",
      "         [-0.9330, -1.4806],\n",
      "         [-0.8644, -1.5328],\n",
      "         [-0.8233, -1.6242],\n",
      "         [-0.8233, -1.7025],\n",
      "         [-0.8096, -1.8069],\n",
      "         [-0.8096, -1.8853],\n",
      "         [-1.4951, -1.9636],\n",
      "         [-1.4540, -2.0028],\n",
      "         [-1.4128, -2.0289],\n",
      "         [-1.3717, -2.0289],\n",
      "         [-1.3306, -2.0028],\n",
      "         [-1.1797, -2.0028],\n",
      "         [-1.1386, -2.0158],\n",
      "         [-1.0975, -2.0158],\n",
      "         [-1.0289, -2.0028],\n",
      "         [-0.9741, -1.9767],\n",
      "         [-1.2483, -1.9114],\n",
      "         [-1.2757, -1.8722],\n",
      "         [-1.2757, -1.8069],\n",
      "         [-1.2757, -1.7678],\n",
      "         [-1.2894, -1.7156],\n",
      "         [-1.2894, -1.7156],\n",
      "         [-1.2483, -1.7156],\n",
      "         [-1.2346, -1.7156],\n",
      "         [-1.1935, -1.7156],\n",
      "         [-1.4403, -1.9114],\n",
      "         [-1.3991, -1.9244],\n",
      "         [-1.3717, -1.9244],\n",
      "         [-1.3306, -1.9114],\n",
      "         [-1.3580, -1.8853],\n",
      "         [-1.3991, -1.8853],\n",
      "         [-1.1523, -1.8983],\n",
      "         [-1.1112, -1.9244],\n",
      "         [-1.0701, -1.9244],\n",
      "         [-1.0289, -1.8983],\n",
      "         [-1.0701, -1.8853],\n",
      "         [-1.1112, -1.8853],\n",
      "         [-1.3580, -1.5719],\n",
      "         [-1.3169, -1.6111],\n",
      "         [-1.2894, -1.6372],\n",
      "         [-1.2483, -1.6372],\n",
      "         [-1.2346, -1.6372],\n",
      "         [-1.1935, -1.6111],\n",
      "         [-1.1386, -1.5981],\n",
      "         [-1.1797, -1.5719],\n",
      "         [-1.2209, -1.5589],\n",
      "         [-1.2483, -1.5589],\n",
      "         [-1.2894, -1.5328],\n",
      "         [-1.3169, -1.5589],\n",
      "         [-1.3306, -1.5981],\n",
      "         [-1.2894, -1.5981],\n",
      "         [-1.2483, -1.6111],\n",
      "         [-1.2209, -1.6111],\n",
      "         [-1.1386, -1.5981],\n",
      "         [-1.2209, -1.5981],\n",
      "         [-1.2483, -1.5719],\n",
      "         [-1.2894, -1.5719]]], dtype=torch.float64)}\n",
      "{'image': tensor([[[[0.0025, 0.0025, 0.0025,  ..., 0.0020, 0.0020, 0.0020],\n",
      "          [0.0025, 0.0024, 0.0024,  ..., 0.0020, 0.0020, 0.0020],\n",
      "          [0.0026, 0.0025, 0.0024,  ..., 0.0020, 0.0020, 0.0020],\n",
      "          ...,\n",
      "          [0.0006, 0.0006, 0.0006,  ..., 0.0005, 0.0005, 0.0005],\n",
      "          [0.0006, 0.0006, 0.0006,  ..., 0.0005, 0.0005, 0.0004],\n",
      "          [0.0006, 0.0006, 0.0006,  ..., 0.0005, 0.0005, 0.0004]]]]), 'keypoints': tensor([[[-2.0151, -1.7076],\n",
      "         [-1.9698, -1.5095],\n",
      "         [-1.8941, -1.3419],\n",
      "         [-1.8488, -1.1895],\n",
      "         [-1.7429, -1.0371],\n",
      "         [-1.6220, -0.9152],\n",
      "         [-1.4707, -0.8390],\n",
      "         [-1.2741, -0.7933],\n",
      "         [-1.0473, -0.7933],\n",
      "         [-0.8507, -0.8695],\n",
      "         [-0.7298, -0.9457],\n",
      "         [-0.6541, -1.0524],\n",
      "         [-0.5785, -1.2048],\n",
      "         [-0.5785, -1.3724],\n",
      "         [-0.5785, -1.5248],\n",
      "         [-0.5785, -1.7229],\n",
      "         [-0.6088, -1.8752],\n",
      "         [-1.8185, -1.8600],\n",
      "         [-1.6976, -1.9362],\n",
      "         [-1.5766, -1.9362],\n",
      "         [-1.5010, -1.9667],\n",
      "         [-1.3951, -1.9667],\n",
      "         [-1.0776, -1.9971],\n",
      "         [-1.0020, -2.0733],\n",
      "         [-0.9263, -2.0733],\n",
      "         [-0.8054, -2.0733],\n",
      "         [-0.7298, -2.0733],\n",
      "         [-1.1985, -1.8448],\n",
      "         [-1.1532, -1.7229],\n",
      "         [-1.1229, -1.6467],\n",
      "         [-1.1229, -1.5248],\n",
      "         [-1.2288, -1.4181],\n",
      "         [-1.1985, -1.4181],\n",
      "         [-1.1229, -1.4181],\n",
      "         [-1.0473, -1.4486],\n",
      "         [-1.0020, -1.4943],\n",
      "         [-1.6673, -1.7686],\n",
      "         [-1.5766, -1.8143],\n",
      "         [-1.5010, -1.8143],\n",
      "         [-1.4254, -1.8143],\n",
      "         [-1.4707, -1.7686],\n",
      "         [-1.5766, -1.7686],\n",
      "         [-1.0473, -1.8752],\n",
      "         [-0.9566, -1.9210],\n",
      "         [-0.8810, -1.9210],\n",
      "         [-0.8054, -1.9210],\n",
      "         [-0.8810, -1.8752],\n",
      "         [-0.9566, -1.8752],\n",
      "         [-1.4254, -1.1895],\n",
      "         [-1.3044, -1.2657],\n",
      "         [-1.1532, -1.2962],\n",
      "         [-1.1229, -1.2962],\n",
      "         [-1.0473, -1.2962],\n",
      "         [-0.9263, -1.2962],\n",
      "         [-0.8507, -1.2962],\n",
      "         [-0.9263, -1.2200],\n",
      "         [-1.0020, -1.1743],\n",
      "         [-1.0776, -1.1438],\n",
      "         [-1.1985, -1.1438],\n",
      "         [-1.2741, -1.1438],\n",
      "         [-1.4254, -1.1895],\n",
      "         [-1.1985, -1.2200],\n",
      "         [-1.1229, -1.2657],\n",
      "         [-1.0473, -1.2505],\n",
      "         [-0.8810, -1.2962],\n",
      "         [-1.0473, -1.2505],\n",
      "         [-1.1229, -1.2200],\n",
      "         [-1.1985, -1.2200]]], dtype=torch.float64)}\n",
      "{'image': tensor([[[[1.4656e-04, 1.8514e-04, 2.2369e-04,  ..., 1.8055e-05,\n",
      "           1.8055e-05, 1.8055e-05],\n",
      "          [1.3558e-04, 1.6962e-04, 2.1262e-04,  ..., 1.8055e-05,\n",
      "           1.8055e-05, 1.8055e-05],\n",
      "          [1.3159e-04, 1.5624e-04, 1.9147e-04,  ..., 1.8055e-05,\n",
      "           1.8055e-05, 1.8055e-05],\n",
      "          ...,\n",
      "          [2.7916e-03, 2.4400e-03, 1.9551e-03,  ..., 1.8055e-05,\n",
      "           1.8055e-05, 1.8055e-05],\n",
      "          [3.1706e-03, 3.1323e-03, 2.8393e-03,  ..., 1.8055e-05,\n",
      "           1.8055e-05, 1.8055e-05],\n",
      "          [2.9903e-03, 3.1454e-03, 3.1964e-03,  ..., 1.8055e-05,\n",
      "           1.8055e-05, 1.8055e-05]]]]), 'keypoints': tensor([[[-2.3246, -3.7525],\n",
      "         [-2.3246, -3.4517],\n",
      "         [-2.2760, -3.2142],\n",
      "         [-2.1465, -2.9925],\n",
      "         [-2.0331, -2.7550],\n",
      "         [-1.7253, -2.5175],\n",
      "         [-1.4338, -2.3908],\n",
      "         [-1.0774, -2.2642],\n",
      "         [-0.7697, -2.2008],\n",
      "         [-0.5429, -2.3117],\n",
      "         [-0.4781, -2.5017],\n",
      "         [-0.4781, -2.6758],\n",
      "         [-0.3647, -2.8500],\n",
      "         [-0.2999, -3.1350],\n",
      "         [-0.2999, -3.3725],\n",
      "         [-0.2999, -3.5942],\n",
      "         [-0.2999, -3.8317],\n",
      "         [-1.6119, -3.9108],\n",
      "         [-1.4338, -4.0217],\n",
      "         [-1.2556, -4.0217],\n",
      "         [-1.0774, -4.0217],\n",
      "         [-0.9478, -4.0217],\n",
      "         [-0.5429, -4.1325],\n",
      "         [-0.4781, -4.1800],\n",
      "         [-0.3647, -4.1800],\n",
      "         [-0.2999, -4.1800],\n",
      "         [-0.2999, -4.1800],\n",
      "         [-0.7211, -3.7842],\n",
      "         [-0.6563, -3.6100],\n",
      "         [-0.5429, -3.4833],\n",
      "         [-0.5429, -3.3092],\n",
      "         [-0.8344, -3.1983],\n",
      "         [-0.7211, -3.1983],\n",
      "         [-0.6563, -3.1983],\n",
      "         [-0.5915, -3.1983],\n",
      "         [-0.5429, -3.2458],\n",
      "         [-1.4338, -3.7367],\n",
      "         [-1.2556, -3.7842],\n",
      "         [-1.1260, -3.7842],\n",
      "         [-1.0774, -3.7842],\n",
      "         [-1.1260, -3.7367],\n",
      "         [-1.2556, -3.6733],\n",
      "         [-0.6563, -3.8317],\n",
      "         [-0.5429, -3.8950],\n",
      "         [-0.4133, -3.9583],\n",
      "         [-0.4133, -3.9583],\n",
      "         [-0.4133, -3.8317],\n",
      "         [-0.5429, -3.8317],\n",
      "         [-1.1260, -2.8025],\n",
      "         [-0.8992, -2.9133],\n",
      "         [-0.7211, -2.9608],\n",
      "         [-0.5915, -2.9608],\n",
      "         [-0.5429, -2.9608],\n",
      "         [-0.4781, -2.9608],\n",
      "         [-0.4781, -2.8975],\n",
      "         [-0.4781, -2.7867],\n",
      "         [-0.5429, -2.7233],\n",
      "         [-0.6563, -2.6758],\n",
      "         [-0.7697, -2.6758],\n",
      "         [-0.8992, -2.7392],\n",
      "         [-1.0774, -2.8500],\n",
      "         [-0.7697, -2.8500],\n",
      "         [-0.6563, -2.8975],\n",
      "         [-0.5915, -2.8975],\n",
      "         [-0.4781, -2.8975],\n",
      "         [-0.5915, -2.8500],\n",
      "         [-0.6563, -2.7867],\n",
      "         [-0.7697, -2.7867]]], dtype=torch.float64)}\n",
      "{'image': tensor([[[[9.2479e-04, 1.0208e-03, 1.1296e-03,  ..., 8.4714e-04,\n",
      "           8.5526e-04, 8.6251e-04],\n",
      "          [8.8088e-04, 1.0009e-03, 1.1213e-03,  ..., 4.1565e-04,\n",
      "           4.4185e-04, 4.9762e-04],\n",
      "          [9.0300e-04, 1.0003e-03, 1.0806e-03,  ..., 9.8614e-05,\n",
      "           1.0388e-04, 1.3305e-04],\n",
      "          ...,\n",
      "          [4.5982e-06, 1.3228e-05, 3.5058e-05,  ..., 1.0013e-03,\n",
      "           7.4745e-04, 6.6125e-04],\n",
      "          [4.8514e-06, 1.2982e-05, 2.3223e-05,  ..., 1.0171e-03,\n",
      "           6.7042e-04, 1.0747e-03],\n",
      "          [1.3487e-05, 4.5982e-06, 4.5982e-06,  ..., 1.0338e-03,\n",
      "           4.0016e-04, 1.3459e-03]]]]), 'keypoints': tensor([[[-1.6342, -2.4426],\n",
      "         [-1.6038, -2.3748],\n",
      "         [-1.5735, -2.2800],\n",
      "         [-1.5583, -2.1987],\n",
      "         [-1.4975, -2.1039],\n",
      "         [-1.4215, -2.0361],\n",
      "         [-1.3608, -2.0090],\n",
      "         [-1.2544, -1.9548],\n",
      "         [-1.1177, -1.9548],\n",
      "         [-1.0113, -2.0090],\n",
      "         [-0.9354, -2.0497],\n",
      "         [-0.8746, -2.1174],\n",
      "         [-0.8138, -2.1987],\n",
      "         [-0.7987, -2.2935],\n",
      "         [-0.7987, -2.3613],\n",
      "         [-0.7987, -2.4561],\n",
      "         [-0.8138, -2.5510],\n",
      "         [-1.5735, -2.5645],\n",
      "         [-1.5279, -2.6323],\n",
      "         [-1.4671, -2.6323],\n",
      "         [-1.4215, -2.6323],\n",
      "         [-1.3608, -2.6323],\n",
      "         [-1.1481, -2.6594],\n",
      "         [-1.0873, -2.6865],\n",
      "         [-1.0113, -2.7000],\n",
      "         [-0.9506, -2.6729],\n",
      "         [-0.9050, -2.6594],\n",
      "         [-1.2240, -2.5103],\n",
      "         [-1.1937, -2.4697],\n",
      "         [-1.1937, -2.3884],\n",
      "         [-1.1937, -2.3477],\n",
      "         [-1.2544, -2.2935],\n",
      "         [-1.2240, -2.2935],\n",
      "         [-1.1785, -2.2935],\n",
      "         [-1.1481, -2.2935],\n",
      "         [-1.1177, -2.3206],\n",
      "         [-1.4671, -2.4968],\n",
      "         [-1.4367, -2.5103],\n",
      "         [-1.3912, -2.5103],\n",
      "         [-1.3304, -2.5103],\n",
      "         [-1.3912, -2.4968],\n",
      "         [-1.4367, -2.4697],\n",
      "         [-1.0873, -2.5374],\n",
      "         [-1.0721, -2.5645],\n",
      "         [-1.0113, -2.5645],\n",
      "         [-0.9506, -2.5510],\n",
      "         [-0.9810, -2.5374],\n",
      "         [-1.0721, -2.5103],\n",
      "         [-1.3152, -2.1716],\n",
      "         [-1.2544, -2.1987],\n",
      "         [-1.1937, -2.2258],\n",
      "         [-1.1785, -2.2258],\n",
      "         [-1.1481, -2.2529],\n",
      "         [-1.0721, -2.2258],\n",
      "         [-1.0417, -2.1987],\n",
      "         [-1.0721, -2.1445],\n",
      "         [-1.1177, -2.1310],\n",
      "         [-1.1481, -2.1310],\n",
      "         [-1.1937, -2.1310],\n",
      "         [-1.2544, -2.1445],\n",
      "         [-1.3152, -2.1716],\n",
      "         [-1.1937, -2.1987],\n",
      "         [-1.1785, -2.1987],\n",
      "         [-1.1177, -2.1987],\n",
      "         [-1.0417, -2.1987],\n",
      "         [-1.1177, -2.1716],\n",
      "         [-1.1785, -2.1716],\n",
      "         [-1.1937, -2.1716]]], dtype=torch.float64)}\n",
      "{'image': tensor([[[[0.0034, 0.0035, 0.0035,  ..., 0.0008, 0.0008, 0.0008],\n",
      "          [0.0031, 0.0031, 0.0031,  ..., 0.0008, 0.0008, 0.0008],\n",
      "          [0.0027, 0.0028, 0.0028,  ..., 0.0007, 0.0007, 0.0007],\n",
      "          ...,\n",
      "          [0.0032, 0.0028, 0.0020,  ..., 0.0011, 0.0011, 0.0012],\n",
      "          [0.0032, 0.0028, 0.0020,  ..., 0.0012, 0.0012, 0.0012],\n",
      "          [0.0031, 0.0029, 0.0020,  ..., 0.0012, 0.0012, 0.0012]]]]), 'keypoints': tensor([[[-3.3584, -1.3227],\n",
      "         [-3.3584, -1.1373],\n",
      "         [-3.3584, -0.9982],\n",
      "         [-3.3155, -0.7973],\n",
      "         [-3.3155, -0.6582],\n",
      "         [-3.2295, -0.5191],\n",
      "         [-3.1436, -0.4109],\n",
      "         [-3.0005, -0.3182],\n",
      "         [-2.8286, -0.2255],\n",
      "         [-2.6568, -0.2564],\n",
      "         [-2.5136, -0.3027],\n",
      "         [-2.4707, -0.4109],\n",
      "         [-2.3848, -0.5036],\n",
      "         [-2.2989, -0.6891],\n",
      "         [-2.2559, -0.8282],\n",
      "         [-2.2130, -0.9673],\n",
      "         [-2.2130, -1.1682],\n",
      "         [-3.1866, -1.4618],\n",
      "         [-3.1007, -1.5082],\n",
      "         [-3.0005, -1.5082],\n",
      "         [-2.9145, -1.5082],\n",
      "         [-2.8286, -1.4618],\n",
      "         [-2.5136, -1.4155],\n",
      "         [-2.4277, -1.4000],\n",
      "         [-2.3418, -1.4000],\n",
      "         [-2.2989, -1.3536],\n",
      "         [-2.2130, -1.3073],\n",
      "         [-2.6998, -1.2609],\n",
      "         [-2.6998, -1.1218],\n",
      "         [-2.6998, -1.0291],\n",
      "         [-2.6998, -0.8900],\n",
      "         [-2.8286, -0.8436],\n",
      "         [-2.7857, -0.8436],\n",
      "         [-2.7427, -0.7973],\n",
      "         [-2.6568, -0.8436],\n",
      "         [-2.6568, -0.8436],\n",
      "         [-3.0577, -1.3227],\n",
      "         [-3.0005, -1.3227],\n",
      "         [-2.9145, -1.3227],\n",
      "         [-2.8716, -1.2764],\n",
      "         [-2.9145, -1.2764],\n",
      "         [-3.0005, -1.2764],\n",
      "         [-2.5709, -1.2145],\n",
      "         [-2.4707, -1.2609],\n",
      "         [-2.3848, -1.2609],\n",
      "         [-2.3418, -1.2145],\n",
      "         [-2.4277, -1.1682],\n",
      "         [-2.4707, -1.2145],\n",
      "         [-2.9575, -0.6582],\n",
      "         [-2.8716, -0.7045],\n",
      "         [-2.7857, -0.6891],\n",
      "         [-2.7427, -0.6891],\n",
      "         [-2.6998, -0.6891],\n",
      "         [-2.6139, -0.6427],\n",
      "         [-2.5709, -0.5964],\n",
      "         [-2.6568, -0.5500],\n",
      "         [-2.6998, -0.5500],\n",
      "         [-2.7427, -0.5036],\n",
      "         [-2.8286, -0.5500],\n",
      "         [-2.9145, -0.5964],\n",
      "         [-2.9575, -0.6582],\n",
      "         [-2.8286, -0.6427],\n",
      "         [-2.7427, -0.6427],\n",
      "         [-2.6998, -0.5964],\n",
      "         [-2.6139, -0.5964],\n",
      "         [-2.6998, -0.5964],\n",
      "         [-2.7427, -0.5964],\n",
      "         [-2.8286, -0.6427]]], dtype=torch.float64)}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "high <= 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      4\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(transformed_dataset, \n\u001b[1;32m      5\u001b[0m                           batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m      6\u001b[0m                           shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m      7\u001b[0m                           num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(item)\n",
      "File \u001b[0;32m~/my_project_dir/my_project_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/my_project_dir/my_project_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    560\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    563\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/my_project_dir/my_project_env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/my_project_dir/my_project_env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/my_project_dir/udacity-facial-keypoint-detection-master/Load.py:43\u001b[0m, in \u001b[0;36mFacialKeypointsDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     40\u001b[0m sample \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: image, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints\u001b[39m\u001b[38;5;124m'\u001b[39m: key_pts}\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 43\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sample\n",
      "File \u001b[0;32m~/my_project_dir/my_project_env/lib/python3.8/site-packages/torchvision/transforms/transforms.py:61\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 61\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/my_project_dir/udacity-facial-keypoint-detection-master/Load.py:127\u001b[0m, in \u001b[0;36mRandomCrop.__call__\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(h \u001b[38;5;241m-\u001b[39m new_h\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m w \u001b[38;5;241m-\u001b[39m new_w):\n\u001b[1;32m    126\u001b[0m     top \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, h \u001b[38;5;241m-\u001b[39m new_h)\n\u001b[0;32m--> 127\u001b[0m     left \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnew_w\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     top \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, h \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32mmtrand.pyx:748\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_bounded_integers.pyx:1247\u001b[0m, in \u001b[0;36mnumpy.random._bounded_integers._rand_int64\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: high <= 0"
     ]
    }
   ],
   "source": [
    "# load training data in batches\n",
    "batch_size = 1\n",
    "\n",
    "train_loader = DataLoader(transformed_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True, \n",
    "                          num_workers=0)\n",
    "for item in train_loader:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before training\n",
    "\n",
    "Take a look at how this model performs before it trains. You should see that the keypoints it predicts start off in one spot and don't match the keypoints on a face at all! It's interesting to visualize this behavior so that you can compare it to the model after training and see how the model has improved.\n",
    "\n",
    "#### Load in the test dataset\n",
    "\n",
    "The test dataset is one that this model has *not* seen before, meaning it has not trained with these images. We'll load in this test data and before and after training, see how our model performs on this set!\n",
    "\n",
    "To visualize this test data, we have to go through some un-transformation steps to turn our images into python images from tensors and to turn our keypoints back into a recognizable range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the test data, using the dataset class\n",
    "# AND apply the data_transform you defined above\n",
    "\n",
    "# create the test dataset\n",
    "test_dataset = FacialKeypointsDataset(csv_file='data/test_frames_keypoints.csv',\n",
    "                                             root_dir='data/test/',\n",
    "                                             transform=data_transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data in batches\n",
    "batch_size = 1\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True, \n",
    "                          num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the model on a test sample\n",
    "\n",
    "To test the model on a test sample of data, we have to follow these steps:\n",
    "1. Extract the image and ground truth keypoints from a sample\n",
    "2. Wrap the image in a Variable, so that the net can process it as input and track how it changes as the image moves through the network.\n",
    "3. Make sure the image is a FloatTensor, which the model expects.\n",
    "4. Forward pass the image through the net to get the predicted, output keypoints.\n",
    "\n",
    "This function test how the network performs on the first batch of test data. It returns the images, the transformed images, the predicted keypoints (produced by the model), and the ground truth keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model on a batch of test images\n",
    "\n",
    "def net_sample_output():\n",
    "    \n",
    "    # iterate through the test dataset\n",
    "    for i, sample in enumerate(test_loader):\n",
    "        \n",
    "        # get sample data: images and ground truth keypoints\n",
    "        images = sample['image']\n",
    "        key_pts = sample['keypoints']\n",
    "\n",
    "        # convert images to FloatTensors\n",
    "        images = images.type(torch.FloatTensor)\n",
    "\n",
    "        # forward pass to get net output\n",
    "        output_pts = net(images)\n",
    "        \n",
    "        # reshape to batch_size x 68 x 2 pts\n",
    "        output_pts = output_pts.view(output_pts.size()[0], 68, -1)\n",
    "        \n",
    "        # break after first image is tested\n",
    "        if i == 0:\n",
    "            return images, output_pts, key_pts\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debugging tips\n",
    "\n",
    "If you get a size or dimension error here, make sure that your network outputs the expected number of keypoints! Or if you get a Tensor type error, look into changing the above code that casts the data into float types: `images = images.type(torch.FloatTensor)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:79] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (128x12x1). Calculated output size: (128x6x0). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# call the above function\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# returns: test images, test predicted keypoints, test ground truth keypoints\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m test_images, test_outputs, gt_pts \u001b[38;5;241m=\u001b[39m \u001b[43mnet_sample_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# print out the dimensions of the data to see if they make sense\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_images\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39msize())\n",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36mnet_sample_output\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mFloatTensor)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# forward pass to get net output\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m output_pts \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# reshape to batch_size x 68 x 2 pts\u001b[39;00m\n\u001b[1;32m     19\u001b[0m output_pts \u001b[38;5;241m=\u001b[39m output_pts\u001b[38;5;241m.\u001b[39mview(output_pts\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m68\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/my_project_dir/my_project_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/my_project_dir/udacity-facial-keypoint-detection-master/models.py:63\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))\n\u001b[1;32m     62\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[0;32m---> 63\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv4(x)))\n\u001b[1;32m     65\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv5(x)))\n",
      "File \u001b[0;32m~/my_project_dir/my_project_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/my_project_dir/my_project_env/lib/python3.8/site-packages/torch/nn/modules/pooling.py:162\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my_project_dir/my_project_env/lib/python3.8/site-packages/torch/_jit_internal.py:422\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my_project_dir/my_project_env/lib/python3.8/site-packages/torch/nn/functional.py:719\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    718\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 719\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (128x12x1). Calculated output size: (128x6x0). Output size is too small"
     ]
    }
   ],
   "source": [
    "# call the above function\n",
    "# returns: test images, test predicted keypoints, test ground truth keypoints\n",
    "test_images, test_outputs, gt_pts = net_sample_output()\n",
    "\n",
    "# print out the dimensions of the data to see if they make sense\n",
    "print(test_images.data.size())\n",
    "print(test_outputs.data.size())\n",
    "print(gt_pts.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the predicted keypoints\n",
    "\n",
    "Once we've had the model produce some predicted output keypoints, we can visualize these points in a way that's similar to how we've displayed this data before, only this time, we have to \"un-transform\" the image/keypoint data to display it.\n",
    "\n",
    "The *new* function, `show_all_keypoints` displays a grayscale image, its predicted keypoints and its ground truth keypoints (if provided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_all_keypoints(image, predicted_key_pts, gt_pts=None):\n",
    "    \"\"\"Show image with predicted keypoints\"\"\"\n",
    "    \n",
    "    # image is grayscale\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.scatter(predicted_key_pts[:, 0], predicted_key_pts[:, 1], s=20, marker='.', c='m')\n",
    "    # plot ground truth points as green pts\n",
    "    if gt_pts is not None:\n",
    "        plt.scatter(gt_pts[:, 0], gt_pts[:, 1], s=20, marker='.', c='g')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Un-transformation\n",
    "\n",
    "Next, you'll see a helper function. `visualize_output` that takes in a batch of images, predicted keypoints, and ground truth keypoints and displays a set of those images and their true/predicted keypoints.\n",
    "\n",
    "This function's main role is to take batches of image and keypoint data (the input and output of your CNN), and transform them into numpy images and un-normalized keypoints (x, y) for normal display. The un-transformation process turns keypoints and images into numpy arrays from Tensors *and* it undoes the keypoint normalization done in the Normalize() transform; it's assumed that you applied these transformations when you loaded your test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the output\n",
    "# by default this shows a batch of 10 images\n",
    "def visualize_output(test_images, test_outputs, gt_pts=None, batch_size=10):\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        plt.figure(figsize=(20,10))\n",
    "        ax = plt.subplot(1, batch_size, i+1)\n",
    "\n",
    "        # un-transform the image data\n",
    "        image = test_images[i].data   # get the image from it's Variable wrapper\n",
    "        image = image.numpy()   # convert to numpy array from a Tensor\n",
    "        image = np.transpose(image, (1, 2, 0))   # transpose to go from torch to numpy image\n",
    "\n",
    "        # un-transform the predicted key_pts data\n",
    "        predicted_key_pts = test_outputs[i].data\n",
    "        predicted_key_pts = predicted_key_pts.numpy()\n",
    "        # undo normalization of keypoints  \n",
    "        predicted_key_pts = predicted_key_pts*50.0+100\n",
    "        \n",
    "        # plot ground truth points for comparison, if they exist\n",
    "        ground_truth_pts = None\n",
    "        if gt_pts is not None:\n",
    "            ground_truth_pts = gt_pts[i]         \n",
    "            ground_truth_pts = ground_truth_pts*50.0+100\n",
    "        \n",
    "        # call show_all_keypoints\n",
    "        show_all_keypoints(np.squeeze(image), predicted_key_pts, ground_truth_pts)\n",
    "            \n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "# call it\n",
    "visualize_output(test_images, test_outputs, gt_pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "#### Loss function\n",
    "Training a network to predict keypoints is different than training a network to predict a class; instead of outputting a distribution of classes and using cross entropy loss, we have to choose a loss function that is suited for regression, which directly compares a predicted value and target value. Read about the various kinds of loss functions (like MSE or L1/SmoothL1 loss) in [this documentation](http://pytorch.org/docs/master/_modules/torch/nn/modules/loss.html).\n",
    "\n",
    "### Define the loss and optimization\n",
    "\n",
    "Next, we will define how the model will train by deciding on the loss function and optimizer.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the loss and optimization\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Initial Observation\n",
    "\n",
    "Now, we will train on our batched training data from `train_loader` for a number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(n_epochs):\n",
    "\n",
    "    # prepare the net for training\n",
    "    net.train()\n",
    "    training_loss = []\n",
    "\n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "        \n",
    "        running_loss = 0.0\n",
    "\n",
    "        # train on batches of data, assumes you already have train_loader\n",
    "        for batch_i, data in enumerate(train_loader):\n",
    "            # get the input images and their corresponding labels\n",
    "            images = data['image']\n",
    "            key_pts = data['keypoints']\n",
    "\n",
    "            # flatten pts\n",
    "            key_pts = key_pts.view(key_pts.size(0), -1)\n",
    "\n",
    "            # convert variables to floats for regression loss\n",
    "            key_pts = key_pts.type(torch.FloatTensor)\n",
    "            images = images.type(torch.FloatTensor)\n",
    "\n",
    "            # forward pass to get outputs\n",
    "            output_pts = net(images)\n",
    "\n",
    "            # calculate the loss between predicted and target keypoints\n",
    "            loss = criterion(output_pts, key_pts)\n",
    "\n",
    "            # zero the parameter (weight) gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # backward pass to calculate the weight gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # print loss statistics\n",
    "            running_loss += loss.item()\n",
    "            if batch_i % 10 == 9:    # print every 10 batches\n",
    "                print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1, batch_i+1, running_loss/10))\n",
    "                running_loss = 0.0\n",
    "        training_loss.append(running_loss)\n",
    "\n",
    "    print('Finished Training')\n",
    "    return training_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 10, Avg. Loss: 0.3359374165534973\n",
      "Epoch: 1, Batch: 20, Avg. Loss: 0.2519407905638218\n",
      "Epoch: 1, Batch: 30, Avg. Loss: 0.19335664361715316\n",
      "Epoch: 1, Batch: 40, Avg. Loss: 0.22232168912887573\n",
      "Epoch: 1, Batch: 50, Avg. Loss: 0.20937469676136972\n",
      "Epoch: 1, Batch: 60, Avg. Loss: 0.17770689800381662\n",
      "Epoch: 1, Batch: 70, Avg. Loss: 0.22189381569623948\n",
      "Epoch: 1, Batch: 80, Avg. Loss: 0.22324044555425643\n",
      "Epoch: 1, Batch: 90, Avg. Loss: 0.2096339724957943\n",
      "Epoch: 1, Batch: 100, Avg. Loss: 0.2514382854104042\n",
      "Epoch: 1, Batch: 110, Avg. Loss: 0.19009072184562684\n",
      "Epoch: 1, Batch: 120, Avg. Loss: 0.17226141765713693\n",
      "Epoch: 1, Batch: 130, Avg. Loss: 0.1690574675798416\n",
      "Epoch: 1, Batch: 140, Avg. Loss: 0.210720930993557\n",
      "Epoch: 1, Batch: 150, Avg. Loss: 0.14154042899608613\n",
      "Epoch: 1, Batch: 160, Avg. Loss: 0.24603203684091568\n",
      "Epoch: 1, Batch: 170, Avg. Loss: 0.21118866205215453\n",
      "Epoch: 1, Batch: 180, Avg. Loss: 0.21770252510905266\n",
      "Epoch: 1, Batch: 190, Avg. Loss: 0.20987769737839698\n",
      "Epoch: 1, Batch: 200, Avg. Loss: 0.19756163731217385\n",
      "Epoch: 1, Batch: 210, Avg. Loss: 0.1958801694214344\n",
      "Epoch: 1, Batch: 220, Avg. Loss: 0.15825993344187736\n",
      "Epoch: 1, Batch: 230, Avg. Loss: 0.21512291580438614\n",
      "Epoch: 1, Batch: 240, Avg. Loss: 0.3075722396373749\n",
      "Epoch: 1, Batch: 250, Avg. Loss: 0.17632614076137543\n",
      "Epoch: 1, Batch: 260, Avg. Loss: 0.17920156344771385\n",
      "Epoch: 1, Batch: 270, Avg. Loss: 0.16490745544433594\n",
      "Epoch: 1, Batch: 280, Avg. Loss: 0.17678966298699378\n",
      "Epoch: 1, Batch: 290, Avg. Loss: 0.25272571444511416\n",
      "Epoch: 1, Batch: 300, Avg. Loss: 0.228555628657341\n",
      "Epoch: 1, Batch: 310, Avg. Loss: 0.2572815425693989\n",
      "Epoch: 1, Batch: 320, Avg. Loss: 0.1426238611340523\n",
      "Epoch: 1, Batch: 330, Avg. Loss: 0.20532610416412353\n",
      "Epoch: 1, Batch: 340, Avg. Loss: 0.20154573321342467\n",
      "Epoch: 2, Batch: 10, Avg. Loss: 0.16582690700888633\n",
      "Epoch: 2, Batch: 20, Avg. Loss: 0.16354051679372789\n",
      "Epoch: 2, Batch: 30, Avg. Loss: 0.17885669991374015\n",
      "Epoch: 2, Batch: 40, Avg. Loss: 0.17904315665364265\n",
      "Epoch: 2, Batch: 50, Avg. Loss: 0.2072277545928955\n",
      "Epoch: 2, Batch: 60, Avg. Loss: 0.18444182872772216\n",
      "Epoch: 2, Batch: 70, Avg. Loss: 0.21542685925960542\n",
      "Epoch: 2, Batch: 80, Avg. Loss: 0.15371143072843552\n",
      "Epoch: 2, Batch: 90, Avg. Loss: 0.177742587774992\n",
      "Epoch: 2, Batch: 100, Avg. Loss: 0.17734068930149077\n",
      "Epoch: 2, Batch: 110, Avg. Loss: 0.20291318818926812\n",
      "Epoch: 2, Batch: 120, Avg. Loss: 0.17567959874868394\n",
      "Epoch: 2, Batch: 130, Avg. Loss: 0.27011689692735674\n",
      "Epoch: 2, Batch: 140, Avg. Loss: 0.177052141726017\n",
      "Epoch: 2, Batch: 150, Avg. Loss: 0.22388211637735367\n",
      "Epoch: 2, Batch: 160, Avg. Loss: 0.1859480306506157\n",
      "Epoch: 2, Batch: 170, Avg. Loss: 0.13935057520866395\n",
      "Epoch: 2, Batch: 180, Avg. Loss: 0.16031137704849244\n",
      "Epoch: 2, Batch: 190, Avg. Loss: 0.18583037704229355\n",
      "Epoch: 2, Batch: 200, Avg. Loss: 0.2050660192966461\n",
      "Epoch: 2, Batch: 210, Avg. Loss: 0.18441865965723991\n",
      "Epoch: 2, Batch: 220, Avg. Loss: 0.1929688796401024\n",
      "Epoch: 2, Batch: 230, Avg. Loss: 0.1791047029197216\n",
      "Epoch: 2, Batch: 240, Avg. Loss: 0.15064347684383392\n",
      "Epoch: 2, Batch: 250, Avg. Loss: 0.16301655471324922\n",
      "Epoch: 2, Batch: 260, Avg. Loss: 0.1706638291478157\n",
      "Epoch: 2, Batch: 270, Avg. Loss: 0.20690744668245314\n",
      "Epoch: 2, Batch: 280, Avg. Loss: 0.21713124662637712\n",
      "Epoch: 2, Batch: 290, Avg. Loss: 0.27854378074407576\n",
      "Epoch: 2, Batch: 300, Avg. Loss: 0.1732009954750538\n",
      "Epoch: 2, Batch: 310, Avg. Loss: 0.19125659614801407\n",
      "Epoch: 2, Batch: 320, Avg. Loss: 0.18154872879385947\n",
      "Epoch: 2, Batch: 330, Avg. Loss: 0.2175106830894947\n",
      "Epoch: 2, Batch: 340, Avg. Loss: 0.16089981347322463\n",
      "Epoch: 3, Batch: 10, Avg. Loss: 0.2551815345883369\n",
      "Epoch: 3, Batch: 20, Avg. Loss: 0.207425794005394\n",
      "Epoch: 3, Batch: 30, Avg. Loss: 0.16124368757009505\n",
      "Epoch: 3, Batch: 40, Avg. Loss: 0.19121011942625046\n",
      "Epoch: 3, Batch: 50, Avg. Loss: 0.16061899065971375\n",
      "Epoch: 3, Batch: 60, Avg. Loss: 0.17213073670864104\n",
      "Epoch: 3, Batch: 70, Avg. Loss: 0.1586048498749733\n",
      "Epoch: 3, Batch: 80, Avg. Loss: 0.16035595163702965\n",
      "Epoch: 3, Batch: 90, Avg. Loss: 0.16645561009645463\n",
      "Epoch: 3, Batch: 100, Avg. Loss: 0.1584063470363617\n",
      "Epoch: 3, Batch: 110, Avg. Loss: 0.18043230772018432\n",
      "Epoch: 3, Batch: 120, Avg. Loss: 0.19611115306615828\n",
      "Epoch: 3, Batch: 130, Avg. Loss: 0.16813031435012818\n",
      "Epoch: 3, Batch: 140, Avg. Loss: 0.18390312269330025\n",
      "Epoch: 3, Batch: 150, Avg. Loss: 0.22134049162268638\n",
      "Epoch: 3, Batch: 160, Avg. Loss: 0.17756979763507844\n",
      "Epoch: 3, Batch: 170, Avg. Loss: 0.18251506239175797\n",
      "Epoch: 3, Batch: 180, Avg. Loss: 0.2693963572382927\n",
      "Epoch: 3, Batch: 190, Avg. Loss: 0.2012387178838253\n",
      "Epoch: 3, Batch: 200, Avg. Loss: 0.19979140236973764\n",
      "Epoch: 3, Batch: 210, Avg. Loss: 0.19381434991955757\n",
      "Epoch: 3, Batch: 220, Avg. Loss: 0.18064885288476945\n",
      "Epoch: 3, Batch: 230, Avg. Loss: 0.20098224878311158\n",
      "Epoch: 3, Batch: 240, Avg. Loss: 0.20206240564584732\n",
      "Epoch: 3, Batch: 250, Avg. Loss: 0.19249356761574746\n",
      "Epoch: 3, Batch: 260, Avg. Loss: 0.15976589769124985\n",
      "Epoch: 3, Batch: 270, Avg. Loss: 0.19491592198610305\n",
      "Epoch: 3, Batch: 280, Avg. Loss: 0.19990048706531524\n",
      "Epoch: 3, Batch: 290, Avg. Loss: 0.17115167006850243\n",
      "Epoch: 3, Batch: 300, Avg. Loss: 0.1899074509739876\n",
      "Epoch: 3, Batch: 310, Avg. Loss: 0.19594932198524476\n",
      "Epoch: 3, Batch: 320, Avg. Loss: 0.1818901017308235\n",
      "Epoch: 3, Batch: 330, Avg. Loss: 0.19504818469285964\n",
      "Epoch: 3, Batch: 340, Avg. Loss: 0.180235655605793\n",
      "Epoch: 4, Batch: 10, Avg. Loss: 0.19391626715660096\n",
      "Epoch: 4, Batch: 20, Avg. Loss: 0.1716318540275097\n",
      "Epoch: 4, Batch: 30, Avg. Loss: 0.16555991917848586\n",
      "Epoch: 4, Batch: 40, Avg. Loss: 0.17780651524662971\n",
      "Epoch: 4, Batch: 50, Avg. Loss: 0.158302453905344\n",
      "Epoch: 4, Batch: 60, Avg. Loss: 0.19448787048459054\n",
      "Epoch: 4, Batch: 70, Avg. Loss: 0.20968063771724701\n",
      "Epoch: 4, Batch: 80, Avg. Loss: 0.16276071593165398\n",
      "Epoch: 4, Batch: 90, Avg. Loss: 0.17388773560523987\n",
      "Epoch: 4, Batch: 100, Avg. Loss: 0.24465807676315307\n",
      "Epoch: 4, Batch: 110, Avg. Loss: 0.16394453197717668\n",
      "Epoch: 4, Batch: 120, Avg. Loss: 0.18036361858248712\n",
      "Epoch: 4, Batch: 130, Avg. Loss: 0.17575535476207732\n",
      "Epoch: 4, Batch: 140, Avg. Loss: 0.14305855557322503\n",
      "Epoch: 4, Batch: 150, Avg. Loss: 0.17114726528525354\n",
      "Epoch: 4, Batch: 160, Avg. Loss: 0.17962394431233406\n",
      "Epoch: 4, Batch: 170, Avg. Loss: 0.23222990855574607\n",
      "Epoch: 4, Batch: 180, Avg. Loss: 0.2290678307414055\n",
      "Epoch: 4, Batch: 190, Avg. Loss: 0.16838719844818115\n",
      "Epoch: 4, Batch: 200, Avg. Loss: 0.20865508839488028\n",
      "Epoch: 4, Batch: 210, Avg. Loss: 0.1820104844868183\n",
      "Epoch: 4, Batch: 220, Avg. Loss: 0.1448197193443775\n",
      "Epoch: 4, Batch: 230, Avg. Loss: 0.18086212426424025\n",
      "Epoch: 4, Batch: 240, Avg. Loss: 0.19600836634635926\n",
      "Epoch: 4, Batch: 250, Avg. Loss: 0.19322027862071992\n",
      "Epoch: 4, Batch: 260, Avg. Loss: 0.1697521351277828\n",
      "Epoch: 4, Batch: 270, Avg. Loss: 0.17720937281847\n",
      "Epoch: 4, Batch: 280, Avg. Loss: 0.1729789160192013\n",
      "Epoch: 4, Batch: 290, Avg. Loss: 0.16378805935382842\n",
      "Epoch: 4, Batch: 300, Avg. Loss: 0.20705901682376862\n",
      "Epoch: 4, Batch: 310, Avg. Loss: 0.1760772630572319\n",
      "Epoch: 4, Batch: 320, Avg. Loss: 0.21334995329380035\n",
      "Epoch: 4, Batch: 330, Avg. Loss: 0.18982721194624902\n",
      "Epoch: 4, Batch: 340, Avg. Loss: 0.27141368612647054\n",
      "Epoch: 5, Batch: 10, Avg. Loss: 0.19052079916000367\n",
      "Epoch: 5, Batch: 20, Avg. Loss: 0.19477929547429085\n",
      "Epoch: 5, Batch: 30, Avg. Loss: 0.18252915740013123\n",
      "Epoch: 5, Batch: 40, Avg. Loss: 0.15583500415086746\n",
      "Epoch: 5, Batch: 50, Avg. Loss: 0.16082347929477692\n",
      "Epoch: 5, Batch: 60, Avg. Loss: 0.16574957817792893\n",
      "Epoch: 5, Batch: 70, Avg. Loss: 0.21536863893270491\n",
      "Epoch: 5, Batch: 80, Avg. Loss: 0.18328604251146316\n",
      "Epoch: 5, Batch: 90, Avg. Loss: 0.1718798168003559\n",
      "Epoch: 5, Batch: 100, Avg. Loss: 0.19900345355272292\n",
      "Epoch: 5, Batch: 110, Avg. Loss: 0.19517794400453567\n",
      "Epoch: 5, Batch: 120, Avg. Loss: 0.1630576267838478\n",
      "Epoch: 5, Batch: 130, Avg. Loss: 0.16437458917498587\n",
      "Epoch: 5, Batch: 140, Avg. Loss: 0.16993064805865288\n",
      "Epoch: 5, Batch: 150, Avg. Loss: 0.12963209077715873\n",
      "Epoch: 5, Batch: 160, Avg. Loss: 0.20564216524362564\n",
      "Epoch: 5, Batch: 170, Avg. Loss: 0.1562960870563984\n",
      "Epoch: 5, Batch: 180, Avg. Loss: 0.1519375666975975\n",
      "Epoch: 5, Batch: 190, Avg. Loss: 0.16964376717805862\n",
      "Epoch: 5, Batch: 200, Avg. Loss: 0.17879931703209878\n",
      "Epoch: 5, Batch: 210, Avg. Loss: 0.23523426577448844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Batch: 220, Avg. Loss: 0.2171165056526661\n",
      "Epoch: 5, Batch: 230, Avg. Loss: 0.1657769113779068\n",
      "Epoch: 5, Batch: 240, Avg. Loss: 0.19373459070920945\n",
      "Epoch: 5, Batch: 250, Avg. Loss: 0.18331035524606704\n",
      "Epoch: 5, Batch: 260, Avg. Loss: 0.20340466722846032\n",
      "Epoch: 5, Batch: 270, Avg. Loss: 0.20506639033555984\n",
      "Epoch: 5, Batch: 280, Avg. Loss: 0.19067778289318085\n",
      "Epoch: 5, Batch: 290, Avg. Loss: 0.17561472356319427\n",
      "Epoch: 5, Batch: 300, Avg. Loss: 0.3288019984960556\n",
      "Epoch: 5, Batch: 310, Avg. Loss: 0.17356661334633827\n",
      "Epoch: 5, Batch: 320, Avg. Loss: 0.17975733652710915\n",
      "Epoch: 5, Batch: 330, Avg. Loss: 0.17582921609282492\n",
      "Epoch: 5, Batch: 340, Avg. Loss: 0.20502685159444808\n",
      "Epoch: 6, Batch: 10, Avg. Loss: 0.2084253467619419\n",
      "Epoch: 6, Batch: 20, Avg. Loss: 0.22085311263799667\n",
      "Epoch: 6, Batch: 30, Avg. Loss: 0.18563256338238715\n",
      "Epoch: 6, Batch: 40, Avg. Loss: 0.16427772715687752\n",
      "Epoch: 6, Batch: 50, Avg. Loss: 0.19524946212768554\n",
      "Epoch: 6, Batch: 60, Avg. Loss: 0.1444060578942299\n",
      "Epoch: 6, Batch: 70, Avg. Loss: 0.19809187203645706\n",
      "Epoch: 6, Batch: 80, Avg. Loss: 0.17793049439787864\n",
      "Epoch: 6, Batch: 90, Avg. Loss: 0.18293678537011146\n",
      "Epoch: 6, Batch: 100, Avg. Loss: 0.16208820641040803\n",
      "Epoch: 6, Batch: 110, Avg. Loss: 0.1547427386045456\n",
      "Epoch: 6, Batch: 120, Avg. Loss: 0.1627578467130661\n",
      "Epoch: 6, Batch: 130, Avg. Loss: 0.15238426811993122\n",
      "Epoch: 6, Batch: 140, Avg. Loss: 0.20766048580408097\n",
      "Epoch: 6, Batch: 150, Avg. Loss: 0.18168293684720993\n",
      "Epoch: 6, Batch: 160, Avg. Loss: 0.1976107195019722\n",
      "Epoch: 6, Batch: 170, Avg. Loss: 0.19407136887311935\n",
      "Epoch: 6, Batch: 180, Avg. Loss: 0.17062662690877914\n",
      "Epoch: 6, Batch: 190, Avg. Loss: 0.219535281509161\n",
      "Epoch: 6, Batch: 200, Avg. Loss: 0.17028120905160904\n",
      "Epoch: 6, Batch: 210, Avg. Loss: 0.1763307459652424\n",
      "Epoch: 6, Batch: 220, Avg. Loss: 0.16194813326001167\n",
      "Epoch: 6, Batch: 230, Avg. Loss: 0.18894099220633506\n",
      "Epoch: 6, Batch: 240, Avg. Loss: 0.25229974314570425\n",
      "Epoch: 6, Batch: 250, Avg. Loss: 0.14984748810529708\n",
      "Epoch: 6, Batch: 260, Avg. Loss: 0.23171362578868865\n",
      "Epoch: 6, Batch: 270, Avg. Loss: 0.1801006242632866\n",
      "Epoch: 6, Batch: 280, Avg. Loss: 0.21148105189204217\n",
      "Epoch: 6, Batch: 290, Avg. Loss: 0.17956141978502274\n",
      "Epoch: 6, Batch: 300, Avg. Loss: 0.1830727256834507\n",
      "Epoch: 6, Batch: 310, Avg. Loss: 0.2174043133854866\n",
      "Epoch: 6, Batch: 320, Avg. Loss: 0.17015218809247018\n",
      "Epoch: 6, Batch: 330, Avg. Loss: 0.1661836735904217\n",
      "Epoch: 6, Batch: 340, Avg. Loss: 0.23010388538241386\n",
      "Epoch: 7, Batch: 10, Avg. Loss: 0.170047078281641\n",
      "Epoch: 7, Batch: 20, Avg. Loss: 0.20972486734390258\n",
      "Epoch: 7, Batch: 30, Avg. Loss: 0.21128900349140167\n",
      "Epoch: 7, Batch: 40, Avg. Loss: 0.18649430051445962\n",
      "Epoch: 7, Batch: 50, Avg. Loss: 0.17845963537693024\n",
      "Epoch: 7, Batch: 60, Avg. Loss: 0.17530625984072684\n",
      "Epoch: 7, Batch: 70, Avg. Loss: 0.19705841913819314\n",
      "Epoch: 7, Batch: 80, Avg. Loss: 0.17459178417921067\n",
      "Epoch: 7, Batch: 90, Avg. Loss: 0.16834503784775734\n",
      "Epoch: 7, Batch: 100, Avg. Loss: 0.16384467519819737\n",
      "Epoch: 7, Batch: 110, Avg. Loss: 0.16274996027350425\n",
      "Epoch: 7, Batch: 120, Avg. Loss: 0.19041841328144074\n",
      "Epoch: 7, Batch: 130, Avg. Loss: 0.19043063819408418\n",
      "Epoch: 7, Batch: 140, Avg. Loss: 0.17398253083229065\n",
      "Epoch: 7, Batch: 150, Avg. Loss: 0.19303869009017943\n",
      "Epoch: 7, Batch: 160, Avg. Loss: 0.18585688397288322\n",
      "Epoch: 7, Batch: 170, Avg. Loss: 0.24486568719148635\n",
      "Epoch: 7, Batch: 180, Avg. Loss: 0.18686942607164383\n",
      "Epoch: 7, Batch: 190, Avg. Loss: 0.18023658990859986\n",
      "Epoch: 7, Batch: 200, Avg. Loss: 0.1744642160832882\n",
      "Epoch: 7, Batch: 210, Avg. Loss: 0.17963255047798157\n",
      "Epoch: 7, Batch: 220, Avg. Loss: 0.20740932524204253\n",
      "Epoch: 7, Batch: 230, Avg. Loss: 0.22521183937788009\n",
      "Epoch: 7, Batch: 240, Avg. Loss: 0.16119420900940895\n",
      "Epoch: 7, Batch: 250, Avg. Loss: 0.17268141806125642\n",
      "Epoch: 7, Batch: 260, Avg. Loss: 0.20462956354022027\n",
      "Epoch: 7, Batch: 270, Avg. Loss: 0.18746267557144164\n",
      "Epoch: 7, Batch: 280, Avg. Loss: 0.14031248390674592\n",
      "Epoch: 7, Batch: 290, Avg. Loss: 0.16827831864356996\n",
      "Epoch: 7, Batch: 300, Avg. Loss: 0.20610831305384636\n",
      "Epoch: 7, Batch: 310, Avg. Loss: 0.20709824413061143\n",
      "Epoch: 7, Batch: 320, Avg. Loss: 0.22429162934422492\n",
      "Epoch: 7, Batch: 330, Avg. Loss: 0.13458460420370102\n",
      "Epoch: 7, Batch: 340, Avg. Loss: 0.17032637670636178\n",
      "Epoch: 8, Batch: 10, Avg. Loss: 0.21239283233880996\n",
      "Epoch: 8, Batch: 20, Avg. Loss: 0.20124797970056535\n",
      "Epoch: 8, Batch: 30, Avg. Loss: 0.2138858065009117\n",
      "Epoch: 8, Batch: 40, Avg. Loss: 0.21561948955059052\n",
      "Epoch: 8, Batch: 50, Avg. Loss: 0.19432734176516533\n",
      "Epoch: 8, Batch: 60, Avg. Loss: 0.18534869998693465\n",
      "Epoch: 8, Batch: 70, Avg. Loss: 0.18407218009233475\n",
      "Epoch: 8, Batch: 80, Avg. Loss: 0.20863328129053116\n",
      "Epoch: 8, Batch: 90, Avg. Loss: 0.22089587077498435\n",
      "Epoch: 8, Batch: 100, Avg. Loss: 0.1953217066824436\n",
      "Epoch: 8, Batch: 110, Avg. Loss: 0.18727338165044785\n",
      "Epoch: 8, Batch: 120, Avg. Loss: 0.18570987060666083\n",
      "Epoch: 8, Batch: 130, Avg. Loss: 0.21429493352770806\n",
      "Epoch: 8, Batch: 140, Avg. Loss: 0.17914980202913283\n",
      "Epoch: 8, Batch: 150, Avg. Loss: 0.1838797077536583\n",
      "Epoch: 8, Batch: 160, Avg. Loss: 0.21005586460232734\n",
      "Epoch: 8, Batch: 170, Avg. Loss: 0.16544020548462868\n",
      "Epoch: 8, Batch: 180, Avg. Loss: 0.21640516370534896\n",
      "Epoch: 8, Batch: 190, Avg. Loss: 0.22412154525518418\n",
      "Epoch: 8, Batch: 200, Avg. Loss: 0.21675375550985337\n",
      "Epoch: 8, Batch: 210, Avg. Loss: 0.14187762662768363\n",
      "Epoch: 8, Batch: 220, Avg. Loss: 0.16989366486668586\n",
      "Epoch: 8, Batch: 230, Avg. Loss: 0.21220322027802468\n",
      "Epoch: 8, Batch: 240, Avg. Loss: 0.19021520763635635\n",
      "Epoch: 8, Batch: 250, Avg. Loss: 0.20672883242368698\n",
      "Epoch: 8, Batch: 260, Avg. Loss: 0.17949283123016357\n",
      "Epoch: 8, Batch: 270, Avg. Loss: 0.22917990982532502\n",
      "Epoch: 8, Batch: 280, Avg. Loss: 0.1713012583553791\n",
      "Epoch: 8, Batch: 290, Avg. Loss: 0.153434357047081\n",
      "Epoch: 8, Batch: 300, Avg. Loss: 0.18883581086993217\n",
      "Epoch: 8, Batch: 310, Avg. Loss: 0.17788247838616372\n",
      "Epoch: 8, Batch: 320, Avg. Loss: 0.16921357661485673\n",
      "Epoch: 8, Batch: 330, Avg. Loss: 0.19072714000940322\n",
      "Epoch: 8, Batch: 340, Avg. Loss: 0.19289155825972557\n",
      "Epoch: 9, Batch: 10, Avg. Loss: 0.20369459688663483\n",
      "Epoch: 9, Batch: 20, Avg. Loss: 0.15042009130120276\n",
      "Epoch: 9, Batch: 30, Avg. Loss: 0.1997478760778904\n",
      "Epoch: 9, Batch: 40, Avg. Loss: 0.17073696181178094\n",
      "Epoch: 9, Batch: 50, Avg. Loss: 0.1600024916231632\n",
      "Epoch: 9, Batch: 60, Avg. Loss: 0.18341698721051217\n",
      "Epoch: 9, Batch: 70, Avg. Loss: 0.1893148422241211\n",
      "Epoch: 9, Batch: 80, Avg. Loss: 0.24609573259949685\n",
      "Epoch: 9, Batch: 90, Avg. Loss: 0.18360237032175064\n",
      "Epoch: 9, Batch: 100, Avg. Loss: 0.18831678628921508\n",
      "Epoch: 9, Batch: 110, Avg. Loss: 0.25362480282783506\n",
      "Epoch: 9, Batch: 120, Avg. Loss: 0.18055743724107742\n",
      "Epoch: 9, Batch: 130, Avg. Loss: 0.1584903035312891\n",
      "Epoch: 9, Batch: 140, Avg. Loss: 0.15806210413575172\n",
      "Epoch: 9, Batch: 150, Avg. Loss: 0.22710728421807289\n",
      "Epoch: 9, Batch: 160, Avg. Loss: 0.16126951798796654\n",
      "Epoch: 9, Batch: 170, Avg. Loss: 0.21228601634502411\n",
      "Epoch: 9, Batch: 180, Avg. Loss: 0.1999272510409355\n",
      "Epoch: 9, Batch: 190, Avg. Loss: 0.1800349809229374\n",
      "Epoch: 9, Batch: 200, Avg. Loss: 0.21559081822633744\n",
      "Epoch: 9, Batch: 210, Avg. Loss: 0.22959450185298919\n",
      "Epoch: 9, Batch: 220, Avg. Loss: 0.1967626079916954\n",
      "Epoch: 9, Batch: 230, Avg. Loss: 0.18435381650924682\n",
      "Epoch: 9, Batch: 240, Avg. Loss: 0.14678714871406556\n",
      "Epoch: 9, Batch: 250, Avg. Loss: 0.2472660481929779\n",
      "Epoch: 9, Batch: 260, Avg. Loss: 0.1919250227510929\n",
      "Epoch: 9, Batch: 270, Avg. Loss: 0.19563896656036378\n",
      "Epoch: 9, Batch: 280, Avg. Loss: 0.21133490204811095\n",
      "Epoch: 9, Batch: 290, Avg. Loss: 0.18216811195015908\n",
      "Epoch: 9, Batch: 300, Avg. Loss: 0.1757390759885311\n",
      "Epoch: 9, Batch: 310, Avg. Loss: 0.22165098041296005\n",
      "Epoch: 9, Batch: 320, Avg. Loss: 0.23472658693790435\n",
      "Epoch: 9, Batch: 330, Avg. Loss: 0.1506293833255768\n",
      "Epoch: 9, Batch: 340, Avg. Loss: 0.14383911490440368\n",
      "Epoch: 10, Batch: 10, Avg. Loss: 0.1565363362431526\n",
      "Epoch: 10, Batch: 20, Avg. Loss: 0.1932090699672699\n",
      "Epoch: 10, Batch: 30, Avg. Loss: 0.27365882620215415\n",
      "Epoch: 10, Batch: 40, Avg. Loss: 0.2273286297917366\n",
      "Epoch: 10, Batch: 50, Avg. Loss: 0.1599545732140541\n",
      "Epoch: 10, Batch: 60, Avg. Loss: 0.18961969912052154\n",
      "Epoch: 10, Batch: 70, Avg. Loss: 0.17232041731476783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Batch: 80, Avg. Loss: 0.21363032162189483\n",
      "Epoch: 10, Batch: 90, Avg. Loss: 0.19739335998892785\n",
      "Epoch: 10, Batch: 100, Avg. Loss: 0.22429664358496665\n",
      "Epoch: 10, Batch: 110, Avg. Loss: 0.15968615859746932\n",
      "Epoch: 10, Batch: 120, Avg. Loss: 0.17639247700572014\n",
      "Epoch: 10, Batch: 130, Avg. Loss: 0.180033440887928\n",
      "Epoch: 10, Batch: 140, Avg. Loss: 0.15491386577486993\n",
      "Epoch: 10, Batch: 150, Avg. Loss: 0.17513110041618346\n",
      "Epoch: 10, Batch: 160, Avg. Loss: 0.19761365056037902\n",
      "Epoch: 10, Batch: 170, Avg. Loss: 0.15280862003564835\n",
      "Epoch: 10, Batch: 180, Avg. Loss: 0.2037037879228592\n",
      "Epoch: 10, Batch: 190, Avg. Loss: 0.2092262625694275\n",
      "Epoch: 10, Batch: 200, Avg. Loss: 0.2349842943251133\n",
      "Epoch: 10, Batch: 210, Avg. Loss: 0.16918335258960723\n",
      "Epoch: 10, Batch: 220, Avg. Loss: 0.1955057203769684\n",
      "Epoch: 10, Batch: 230, Avg. Loss: 0.22907571494579315\n",
      "Epoch: 10, Batch: 240, Avg. Loss: 0.2003783881664276\n",
      "Epoch: 10, Batch: 250, Avg. Loss: 0.23790448978543283\n",
      "Epoch: 10, Batch: 260, Avg. Loss: 0.16382805332541467\n",
      "Epoch: 10, Batch: 270, Avg. Loss: 0.1639222651720047\n",
      "Epoch: 10, Batch: 280, Avg. Loss: 0.18137821480631827\n",
      "Epoch: 10, Batch: 290, Avg. Loss: 0.19096504151821136\n",
      "Epoch: 10, Batch: 300, Avg. Loss: 0.21175912916660308\n",
      "Epoch: 10, Batch: 310, Avg. Loss: 0.20187804400920867\n",
      "Epoch: 10, Batch: 320, Avg. Loss: 0.25666377395391465\n",
      "Epoch: 10, Batch: 330, Avg. Loss: 0.20289845988154412\n",
      "Epoch: 10, Batch: 340, Avg. Loss: 0.18463073223829268\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# train your network\n",
    "# n_epochs = 10 # start small, and increase when you've decided on your model structure and hyperparams\n",
    "\n",
    "# this is a Workspaces-specific context manager to keep the connection\n",
    "# alive while training your model, not part of pytorch\n",
    "training_loss = train_net(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the loss as the network trained\n",
    "plt.figure()\n",
    "plt.semilogy(training_loss)\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data\n",
    "\n",
    "See how the model performs on previously unseen, test data. We've already loaded and transformed this data, similar to the training data. Next, run the trained model on these images to see what kind of keypoints are produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sample of test data again\n",
    "test_images, test_outputs, gt_pts = net_sample_output()\n",
    "\n",
    "print(test_images.data.size())\n",
    "print(test_outputs.data.size())\n",
    "print(gt_pts.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize  test output\n",
    "# you can use the same function as before, by un-commenting the line below:\n",
    "\n",
    "visualize_output(test_images, test_outputs, gt_pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have found a good model (or two), we have to save the model so we can load it and use it later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## change the name to something uniqe for each new model\n",
    "model_dir = 'saved_models/'\n",
    "model_name = 'facial_keypoints_model.pt'\n",
    "\n",
    "# after training, save your model parameters in the dir 'saved_models'\n",
    "torch.save(net.state_dict(), model_dir+model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Visualization\n",
    "\n",
    "Sometimes, neural networks are thought of as a black box, given some input, they learn to produce some output. CNN's are actually learning to recognize a variety of spatial patterns and you can visualize what each convolutional layer has been trained to recognize by looking at the weights that make up each convolutional kernel and applying those one at a time to a sample image. This technique is called feature visualization and it's useful for understanding the inner workings of a CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, you can see how to extract a single filter (by index) from your first convolutional layer. The filter should appear as a grayscale grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights in the first conv layer, \"conv1\"\n",
    "# if necessary, change this to reflect the name of your first conv layer\n",
    "weights1 = net.conv1.weight.data\n",
    "\n",
    "w = weights1.numpy()\n",
    "\n",
    "filter_index = 0\n",
    "\n",
    "print(w[filter_index][0])\n",
    "print(w[filter_index][0].shape)\n",
    "\n",
    "# display the filter weights\n",
    "plt.imshow(w[filter_index][0], cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature maps\n",
    "\n",
    "Each CNN has at least one convolutional layer that is composed of stacked filters (also known as convolutional kernels). As a CNN trains, it learns what weights to include in it's convolutional kernels and when these kernels are applied to some input image, they produce a set of **feature maps**. So, feature maps are just sets of filtered images; they are the images produced by applying a convolutional kernel to an input image. These maps show us the features that the different layers of the neural network learn to extract. For example, you might imagine a convolutional kernel that detects the vertical edges of a face or another one that detects the corners of eyes. You can see what kind of features each of these kernels detects by applying them to an image. One such example is shown below; from the way it brings out the lines in an the image, you might characterize this as an edge detection filter.\n",
    "\n",
    "<img src='images/feature_map_ex.png' width=50% height=50%/>\n",
    "\n",
    "\n",
    "Next, choose a test image and filter it with one of the convolutional kernels in your trained CNN; look at the filtered output to get an idea what that particular kernel detects.\n",
    "\n",
    "### Filter an image to see the effect of a convolutional kernel\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load in and display any image from the transformed test dataset\n",
    "import cv2\n",
    "\n",
    "image = cv2.imread('images/mona_lisa.jpg')\n",
    "# convert image to grayscale\n",
    "image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using cv's filter2D function\n",
    "filter_kernel = np.array([[ 0,  1,  1],\n",
    "                          [-1,  0,  1],\n",
    "                          [-1, -1,  0]])\n",
    "\n",
    "filtered_image = cv2.filter2D(image, -1, filter_kernel)\n",
    "\n",
    "f, (ax1, ax2, ax3) = plt.subplots(ncols=3, nrows=1, figsize=(10, 5))\n",
    "ax1.imshow(filter_kernel, cmap='gray')\n",
    "ax2.imshow(image, cmap='gray')\n",
    "ax3.imshow(filtered_image, cmap='gray')\n",
    "\n",
    "ax1.set_title('Kernel')\n",
    "ax2.set_title('Orginal Image')\n",
    "ax3.set_title('Filtered image')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply a specific set of filter weights (like the one displayed above) to the test image\n",
    "weights = net.conv1.weight.data.numpy()\n",
    "\n",
    "filter_kernel = weights[filter_index][0]\n",
    "filtered_image = cv2.filter2D(image, -1, filter_kernel)\n",
    "\n",
    "f, (ax1, ax2, ax3) = plt.subplots(ncols=3, nrows=1, figsize=(10, 5))\n",
    "ax1.imshow(filter_kernel, cmap='gray')\n",
    "ax2.imshow(image, cmap='gray')\n",
    "ax3.imshow(filtered_image, cmap='gray')\n",
    "\n",
    "ax1.set_title('Kernel')\n",
    "ax2.set_title('Orginal Image')\n",
    "ax3.set_title('Filtered image')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Moving on!\n",
    "\n",
    "Now that we have defined and trained the model (and saved the best model), we are ready to move on to the last notebook, which combines a face detector with your saved model to create a facial keypoint detection system that can predict the keypoints on *any* face in an image!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
