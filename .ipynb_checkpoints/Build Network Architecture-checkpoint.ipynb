{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3103d4e6",
   "metadata": {},
   "source": [
    "## Define the Convolutional Neural Network\n",
    "\n",
    "In this notebook and in `models.py`:\n",
    "1. Define a CNN with images as input and keypoints as output\n",
    "2. Construct the transformed FaceKeypointsDataset, just as before\n",
    "3. Train the CNN on the training data, tracking loss\n",
    "4. See how the trained model performs on test data\n",
    "5. If necessary, modify the CNN structure and model hyperparameters, so that it performs well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2cd054",
   "metadata": {},
   "source": [
    "## CNN Architecture\n",
    "\n",
    "Recall that CNN's are defined by a few types of layers:\n",
    "* Convolutional layers\n",
    "* Maxpooling layers\n",
    "* Fully-connected layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13e0d4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from workspace_utils import active_session\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from models import Net\n",
    "\n",
    "from torch.utils.data import DataLoader , Dataset\n",
    "from torchvision import transforms , utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "289ffa3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv5): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=18432, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (fc3): Linear(in_features=512, out_features=136, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_model = Net()\n",
    "print(cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27840d45",
   "metadata": {},
   "source": [
    "## create transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62b6a1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Load import FacialKeypointsDataset\n",
    "from Load import Normalize , RandomCrop , Rescale , ToTensor\n",
    "transform = transforms.Compose([\n",
    "    Normalize(),\n",
    "    Rescale(220),\n",
    "    RandomCrop(170),\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7678ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images:  3462\n",
      "0 torch.Size([1, 73, 109]) torch.Size([68, 2])\n",
      "1 torch.Size([1, 79, 22]) torch.Size([68, 2])\n",
      "2 torch.Size([1, 63, 28]) torch.Size([68, 2])\n",
      "3 torch.Size([1, 18, 79]) torch.Size([68, 2])\n"
     ]
    }
   ],
   "source": [
    "transformed_dataset = FacialKeypointsDataset(csv_file='data/training_frames_keypoints.csv',\n",
    "                                             root_dir='data/training/',\n",
    "                                             transform=transform)\n",
    "\n",
    "print('Number of images: ', len(transformed_dataset))\n",
    "\n",
    "# iterate through the transformed dataset and print some stats about the first few samples\n",
    "for i in range(4):\n",
    "    sample = transformed_dataset[i]\n",
    "    print(i, sample['image'].size(), sample['keypoints'].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b828f7bf",
   "metadata": {},
   "source": [
    "## define train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "004bcc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_leader = DataLoader(transformed_dataset\n",
    "                          , batch_size=10\n",
    "                          , shuffle=True\n",
    "                          , num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ad3bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = FacialKeypointsDataset(csv_file='data/test_frames_keypoints.csv',\n",
    "                                             root_dir='data/test/',\n",
    "                                             transform=transform)\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                          batch_size=10,\n",
    "                          shuffle=True, \n",
    "                          num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e141caa1",
   "metadata": {},
   "source": [
    "## define training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d45f6d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(),lr=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336ed233",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
